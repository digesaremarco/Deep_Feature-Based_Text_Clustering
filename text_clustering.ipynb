{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Deep Feature-Based Text Clustering and Its Explanation\n",
    "\n",
    "This notebook is a reproduction of the paper *\"Deep Feature-Based Text Clustering and its Explanation\"* by Guan et al. (IEEE TKDE, 2022).\n",
    "\n",
    "The paper addresses the limitations of traditional text clustering approaches, which are usually based on the bag-of-words representation and suffer from high dimensionality, sparsity, and lack of contextual/sequence information.\n",
    "\n",
    "The authors propose a novel framework called **Deep Feature-Based Text Clustering (DFTC)** that leverages pretrained deep text encoders (ELMo and InferSent) to generate contextualized sentence/document embeddings. These embeddings are then normalized and clustered using classical algorithms such as K-means.\n",
    "\n",
    "Additionally, the paper introduces the **Text Clustering Results Explanation (TCRE)** module, which applies a logistic regression model on bag-of-words features with pseudo-labels derived from clustering. This allows the extraction of *indication words* that explain the semantics of each cluster, providing interpretability and qualitative evaluation of the results.\n",
    "\n",
    "Experiments on multiple benchmark datasets (AG News, DBpedia, Yahoo! Answers, Reuters) demonstrate that the proposed framework outperforms traditional clustering methods (tf-idf+KMeans, LDA, GSDMM), deep clustering models (DEC, IDEC, STC), and even BERT in most cases. The combination of **deep semantic features + interpretability** makes DFTC an effective and transparent solution for unsupervised text clustering.\n"
   ],
   "id": "ecdac3a081b5a449"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T09:31:55.994857Z",
     "start_time": "2025-09-04T09:31:45.127684Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Core Python ---\n",
    "import os\n",
    "import io\n",
    "import re\n",
    "import zipfile\n",
    "import requests\n",
    "\n",
    "# --- Scientific & ML libraries ---\n",
    "import numpy as np\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "# --- NLP ---\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')  # for Italian\n",
    "\n",
    "# --- Machine Learning & Clustering ---\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "# --- Visualization ---\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# --- Datasets ---\n",
    "from datasets import load_dataset\n"
   ],
   "id": "de5c5109e5472ce1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Università\\LabDM\\.venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T09:33:29.748041Z",
     "start_time": "2025-09-04T09:31:58.750549Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load pre trained ELMo model\n",
    "\n",
    "elmo = hub.load(\"https://tfhub.dev/google/elmo/3\")\n",
    "print(elmo.signatures['default'].structured_outputs)"
   ],
   "id": "ddd0c688ce514154",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Università\\LabDM\\.venv\\Lib\\site-packages\\tensorflow_hub\\resolver.py:120: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Università\\LabDM\\.venv\\Lib\\site-packages\\tensorflow_hub\\resolver.py:120: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 3\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# Load pre trained ELMo model\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m elmo = \u001B[43mhub\u001B[49m\u001B[43m.\u001B[49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mhttps://tfhub.dev/google/elmo/3\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m      4\u001B[39m \u001B[38;5;28mprint\u001B[39m(elmo.signatures[\u001B[33m'\u001B[39m\u001B[33mdefault\u001B[39m\u001B[33m'\u001B[39m].structured_outputs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\Università\\LabDM\\.venv\\Lib\\site-packages\\tensorflow_hub\\module_v2.py:100\u001B[39m, in \u001B[36mload\u001B[39m\u001B[34m(handle, tags, options)\u001B[39m\n\u001B[32m     98\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(handle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[32m     99\u001B[39m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[33m\"\u001B[39m\u001B[33mExpected a string, got \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[33m\"\u001B[39m % handle)\n\u001B[32m--> \u001B[39m\u001B[32m100\u001B[39m module_path = \u001B[43mresolve\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    101\u001B[39m is_hub_module_v1 = tf.io.gfile.exists(_get_module_proto_path(module_path))\n\u001B[32m    102\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m tags \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m is_hub_module_v1:\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\Università\\LabDM\\.venv\\Lib\\site-packages\\tensorflow_hub\\module_v2.py:55\u001B[39m, in \u001B[36mresolve\u001B[39m\u001B[34m(handle)\u001B[39m\n\u001B[32m     31\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mresolve\u001B[39m(handle):\n\u001B[32m     32\u001B[39m \u001B[38;5;250m  \u001B[39m\u001B[33;03m\"\"\"Resolves a module handle into a path.\u001B[39;00m\n\u001B[32m     33\u001B[39m \n\u001B[32m     34\u001B[39m \u001B[33;03m  This function works both for plain TF2 SavedModels and the legacy TF1 Hub\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m     53\u001B[39m \u001B[33;03m    A string representing the Module path.\u001B[39;00m\n\u001B[32m     54\u001B[39m \u001B[33;03m  \"\"\"\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m55\u001B[39m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mregistry\u001B[49m\u001B[43m.\u001B[49m\u001B[43mresolver\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\Università\\LabDM\\.venv\\Lib\\site-packages\\tensorflow_hub\\registry.py:49\u001B[39m, in \u001B[36mMultiImplRegister.__call__\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m     47\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m impl \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mreversed\u001B[39m(\u001B[38;5;28mself\u001B[39m._impls):\n\u001B[32m     48\u001B[39m   \u001B[38;5;28;01mif\u001B[39;00m impl.is_supported(*args, **kwargs):\n\u001B[32m---> \u001B[39m\u001B[32m49\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mimpl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     50\u001B[39m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m     51\u001B[39m     fails.append(\u001B[38;5;28mtype\u001B[39m(impl).\u001B[34m__name__\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\Università\\LabDM\\.venv\\Lib\\site-packages\\tensorflow_hub\\compressed_module_resolver.py:81\u001B[39m, in \u001B[36mHttpCompressedFileResolver.__call__\u001B[39m\u001B[34m(self, handle)\u001B[39m\n\u001B[32m     77\u001B[39m   response = \u001B[38;5;28mself\u001B[39m._call_urlopen(request)\n\u001B[32m     78\u001B[39m   \u001B[38;5;28;01mreturn\u001B[39;00m resolver.DownloadManager(handle).download_and_uncompress(\n\u001B[32m     79\u001B[39m       response, tmp_dir)\n\u001B[32m---> \u001B[39m\u001B[32m81\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mresolver\u001B[49m\u001B[43m.\u001B[49m\u001B[43matomic_download\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdownload\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodule_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     82\u001B[39m \u001B[43m                                \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_lock_file_timeout_sec\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\Università\\LabDM\\.venv\\Lib\\site-packages\\tensorflow_hub\\resolver.py:421\u001B[39m, in \u001B[36matomic_download\u001B[39m\u001B[34m(handle, download_fn, module_dir, lock_file_timeout_sec)\u001B[39m\n\u001B[32m    419\u001B[39m logging.info(\u001B[33m\"\u001B[39m\u001B[33mDownloading TF-Hub Module \u001B[39m\u001B[33m'\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[33m'\u001B[39m\u001B[33m.\u001B[39m\u001B[33m\"\u001B[39m, handle)\n\u001B[32m    420\u001B[39m tf.compat.v1.gfile.MakeDirs(tmp_dir)\n\u001B[32m--> \u001B[39m\u001B[32m421\u001B[39m \u001B[43mdownload_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtmp_dir\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    422\u001B[39m \u001B[38;5;66;03m# Write module descriptor to capture information about which module was\u001B[39;00m\n\u001B[32m    423\u001B[39m \u001B[38;5;66;03m# downloaded by whom and when. The file stored at the same level as a\u001B[39;00m\n\u001B[32m    424\u001B[39m \u001B[38;5;66;03m# directory in order to keep the content of the 'model_dir' exactly as it\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    429\u001B[39m \u001B[38;5;66;03m# module caching protocol and no code in the TF-Hub library reads its\u001B[39;00m\n\u001B[32m    430\u001B[39m \u001B[38;5;66;03m# content.\u001B[39;00m\n\u001B[32m    431\u001B[39m _write_module_descriptor_file(handle, module_dir)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\Università\\LabDM\\.venv\\Lib\\site-packages\\tensorflow_hub\\compressed_module_resolver.py:78\u001B[39m, in \u001B[36mHttpCompressedFileResolver.__call__.<locals>.download\u001B[39m\u001B[34m(handle, tmp_dir)\u001B[39m\n\u001B[32m     75\u001B[39m request = urllib.request.Request(\n\u001B[32m     76\u001B[39m     \u001B[38;5;28mself\u001B[39m._append_compressed_format_query(handle))\n\u001B[32m     77\u001B[39m response = \u001B[38;5;28mself\u001B[39m._call_urlopen(request)\n\u001B[32m---> \u001B[39m\u001B[32m78\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mresolver\u001B[49m\u001B[43m.\u001B[49m\u001B[43mDownloadManager\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdownload_and_uncompress\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     79\u001B[39m \u001B[43m    \u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtmp_dir\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\Università\\LabDM\\.venv\\Lib\\site-packages\\tensorflow_hub\\resolver.py:192\u001B[39m, in \u001B[36mDownloadManager.download_and_uncompress\u001B[39m\u001B[34m(self, fileobj, dst_path)\u001B[39m\n\u001B[32m    182\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Streams the content for the 'fileobj' and stores the result in dst_path.\u001B[39;00m\n\u001B[32m    183\u001B[39m \n\u001B[32m    184\u001B[39m \u001B[33;03mArgs:\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    189\u001B[39m \u001B[33;03m  ValueError: Unknown object encountered inside the TAR file.\u001B[39;00m\n\u001B[32m    190\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    191\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m192\u001B[39m   \u001B[43mfile_utils\u001B[49m\u001B[43m.\u001B[49m\u001B[43mextract_tarfile_to_destination\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    193\u001B[39m \u001B[43m      \u001B[49m\u001B[43mfileobj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdst_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlog_function\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_log_progress\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    194\u001B[39m   total_size_str = tf_utils.bytes_to_readable_str(\n\u001B[32m    195\u001B[39m       \u001B[38;5;28mself\u001B[39m._total_bytes_downloaded, \u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m    196\u001B[39m   \u001B[38;5;28mself\u001B[39m._print_download_progress_msg(\n\u001B[32m    197\u001B[39m       \u001B[33m\"\u001B[39m\u001B[33mDownloaded \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[33m, Total size: \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[33m\"\u001B[39m % (\u001B[38;5;28mself\u001B[39m._url, total_size_str),\n\u001B[32m    198\u001B[39m       flush=\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\Università\\LabDM\\.venv\\Lib\\site-packages\\tensorflow_hub\\file_utils.py:52\u001B[39m, in \u001B[36mextract_tarfile_to_destination\u001B[39m\u001B[34m(fileobj, dst_path, log_function)\u001B[39m\n\u001B[32m     49\u001B[39m abs_target_path = merge_relative_path(dst_path, tarinfo.name)\n\u001B[32m     51\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m tarinfo.isfile():\n\u001B[32m---> \u001B[39m\u001B[32m52\u001B[39m   \u001B[43mextract_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtgz\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarinfo\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mabs_target_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlog_function\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlog_function\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     53\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m tarinfo.isdir():\n\u001B[32m     54\u001B[39m   tf.compat.v1.gfile.MakeDirs(abs_target_path)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\Università\\LabDM\\.venv\\Lib\\site-packages\\tensorflow_hub\\file_utils.py:35\u001B[39m, in \u001B[36mextract_file\u001B[39m\u001B[34m(tgz, tarinfo, dst_path, buffer_size, log_function)\u001B[39m\n\u001B[32m     33\u001B[39m dst = tf.compat.v1.gfile.GFile(dst_path, \u001B[33m\"\u001B[39m\u001B[33mwb\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     34\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[32m1\u001B[39m:\n\u001B[32m---> \u001B[39m\u001B[32m35\u001B[39m   buf = \u001B[43msrc\u001B[49m\u001B[43m.\u001B[49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbuffer_size\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     36\u001B[39m   \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m buf:\n\u001B[32m     37\u001B[39m     \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\Python311\\Lib\\tarfile.py:696\u001B[39m, in \u001B[36m_FileInFile.readinto\u001B[39m\u001B[34m(self, b)\u001B[39m\n\u001B[32m    695\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mreadinto\u001B[39m(\u001B[38;5;28mself\u001B[39m, b):\n\u001B[32m--> \u001B[39m\u001B[32m696\u001B[39m     buf = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    697\u001B[39m     b[:\u001B[38;5;28mlen\u001B[39m(buf)] = buf\n\u001B[32m    698\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(buf)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\Python311\\Lib\\tarfile.py:685\u001B[39m, in \u001B[36m_FileInFile.read\u001B[39m\u001B[34m(self, size)\u001B[39m\n\u001B[32m    683\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m data:\n\u001B[32m    684\u001B[39m     \u001B[38;5;28mself\u001B[39m.fileobj.seek(offset + (\u001B[38;5;28mself\u001B[39m.position - start))\n\u001B[32m--> \u001B[39m\u001B[32m685\u001B[39m     b = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mfileobj\u001B[49m\u001B[43m.\u001B[49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlength\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    686\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(b) != length:\n\u001B[32m    687\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m ReadError(\u001B[33m\"\u001B[39m\u001B[33munexpected end of data\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\Python311\\Lib\\tarfile.py:522\u001B[39m, in \u001B[36m_Stream.read\u001B[39m\u001B[34m(self, size)\u001B[39m\n\u001B[32m    520\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Return the next size number of bytes from the stream.\"\"\"\u001B[39;00m\n\u001B[32m    521\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m size \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m522\u001B[39m buf = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43msize\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    523\u001B[39m \u001B[38;5;28mself\u001B[39m.pos += \u001B[38;5;28mlen\u001B[39m(buf)\n\u001B[32m    524\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m buf\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\Python311\\Lib\\tarfile.py:540\u001B[39m, in \u001B[36m_Stream._read\u001B[39m\u001B[34m(self, size)\u001B[39m\n\u001B[32m    538\u001B[39m     \u001B[38;5;28mself\u001B[39m.buf = \u001B[33mb\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    539\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m540\u001B[39m     buf = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mfileobj\u001B[49m\u001B[43m.\u001B[49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mbufsize\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    541\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m buf:\n\u001B[32m    542\u001B[39m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\Python311\\Lib\\http\\client.py:465\u001B[39m, in \u001B[36mHTTPResponse.read\u001B[39m\u001B[34m(self, amt)\u001B[39m\n\u001B[32m    462\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.length \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m amt > \u001B[38;5;28mself\u001B[39m.length:\n\u001B[32m    463\u001B[39m     \u001B[38;5;66;03m# clip the read to the \"end of response\"\u001B[39;00m\n\u001B[32m    464\u001B[39m     amt = \u001B[38;5;28mself\u001B[39m.length\n\u001B[32m--> \u001B[39m\u001B[32m465\u001B[39m s = \u001B[38;5;28mself\u001B[39m.fp.read(amt)\n\u001B[32m    466\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m s \u001B[38;5;129;01mand\u001B[39;00m amt:\n\u001B[32m    467\u001B[39m     \u001B[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001B[39;00m\n\u001B[32m    468\u001B[39m     \u001B[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001B[39;00m\n\u001B[32m    469\u001B[39m     \u001B[38;5;28mself\u001B[39m._close_conn()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\Python311\\Lib\\socket.py:705\u001B[39m, in \u001B[36mSocketIO.readinto\u001B[39m\u001B[34m(self, b)\u001B[39m\n\u001B[32m    703\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[32m    704\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m705\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_sock\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrecv_into\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    706\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m timeout:\n\u001B[32m    707\u001B[39m         \u001B[38;5;28mself\u001B[39m._timeout_occurred = \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\Python311\\Lib\\ssl.py:1278\u001B[39m, in \u001B[36mSSLSocket.recv_into\u001B[39m\u001B[34m(self, buffer, nbytes, flags)\u001B[39m\n\u001B[32m   1274\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m flags != \u001B[32m0\u001B[39m:\n\u001B[32m   1275\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m   1276\u001B[39m           \u001B[33m\"\u001B[39m\u001B[33mnon-zero flags not allowed in calls to recv_into() on \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[33m\"\u001B[39m %\n\u001B[32m   1277\u001B[39m           \u001B[38;5;28mself\u001B[39m.\u001B[34m__class__\u001B[39m)\n\u001B[32m-> \u001B[39m\u001B[32m1278\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnbytes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbuffer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1279\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1280\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\Python311\\Lib\\ssl.py:1134\u001B[39m, in \u001B[36mSSLSocket.read\u001B[39m\u001B[34m(self, len, buffer)\u001B[39m\n\u001B[32m   1132\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m   1133\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m buffer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1134\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_sslobj\u001B[49m\u001B[43m.\u001B[49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbuffer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1135\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1136\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._sslobj.read(\u001B[38;5;28mlen\u001B[39m)\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Clone git repository\n",
    "!git clone https://github.com/facebookresearch/InferSent.git\n",
    "\n",
    "# Open the InferSent directory\n",
    "%cd InferSent\n",
    "\n",
    "# Install dependencies\n",
    "!pip install torch torchvision nltk\n",
    "\n",
    "# Download the pre-trained InferSent model\n",
    "!wget https://dl.fbaipublicfiles.com/infersent/infersent2.pkl\n",
    "\n",
    "# Download word embeddings (GloVe)\n",
    "!wget http://nlp.stanford.edu/data/glove.840B.300d.zip\n",
    "!unzip -q glove.840B.300d.zip"
   ],
   "id": "23223b73799c5530"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from models import InferSent\n",
    "\n",
    "# Model parameters\n",
    "MODEL_PATH = 'infersent2.pkl'\n",
    "params_model = {\n",
    "    'bsize': 64,\n",
    "    'word_emb_dim': 300,\n",
    "    'enc_lstm_dim': 2048,\n",
    "    'pool_type': 'max',\n",
    "    'dpout_model': 0.0,\n",
    "    'version': 2\n",
    "}\n",
    "infersent = InferSent(params_model)\n",
    "infersent.load_state_dict(torch.load(MODEL_PATH))\n",
    "\n",
    "# GloVe embeddings path\n",
    "W2V_PATH = 'glove.840B.300d.txt'\n",
    "infersent.set_w2v_path(W2V_PATH)"
   ],
   "id": "2af5d1022de38708"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 1: Feature Construction\n",
    "\n",
    "In this step, we transform the input documents into **deep feature representations** using two pretrained models: **ELMo** and **InferSent**.\n",
    "\n",
    "- **ELMo (Language Model based on BiLSTM)**\n",
    "  Provides contextualized word embeddings. To obtain a fixed-size vector for a document, we apply pooling operations over token-level embeddings (e.g., mean-pooling, max-pooling).\n",
    "\n",
    "- **InferSent (Supervised NLI sentence encoder)**\n",
    "  Produces high-quality sentence embeddings using a BiLSTM + max-pooling architecture. For documents with multiple sentences, we compute the average of sentence embeddings.\n",
    "\n",
    "The result of this step is a matrix **X** of shape `(n_docs, d)`, where `d = 1024` for ELMo or `d = 4096` for InferSent.\n",
    "These vectors will later be normalized and clustered (e.g., with K-means).\n",
    "\n",
    "---\n",
    "\n",
    "### DFTC framework overview\n",
    "\n",
    "Below is the overall architecture of the proposed framework from the paper:\n",
    "\n",
    "![DFTC Framework](DFTC_framework.png)\n",
    "\n",
    "*Figure: Deep Feature-Based Text Clustering (DFTC) framework. First, pretrained encoders generate document embeddings. Then, features are normalized and clustered. Finally, the TCRE module explains the clusters by identifying indication words.*\n"
   ],
   "id": "4d5c93c605e3d876"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def extract_features(texts, model='elmo', pooling='mean', batch_size=64):\n",
    "    \"\"\"\n",
    "    Extract deep feature representations for a list of texts using ELMo or InferSent.\n",
    "\n",
    "    Args:\n",
    "        texts (list of str): List of input documents.\n",
    "        model (str): 'elmo' or 'infersent' to choose the embedding model.\n",
    "        pooling (str): Pooling strategy for ELMo ('mean', 'max', 'last').\n",
    "        batch_size (int): Batch size for processing texts with ELMo.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Matrix of shape (n_docs, d) with document embeddings.\n",
    "    \"\"\"\n",
    "    if model == 'elmo':\n",
    "        doc_embeddings = []\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i : i + batch_size]\n",
    "            batch_tokens = [word_tokenize(text) for text in batch_texts]\n",
    "\n",
    "            # Pad sequences to the maximum length in the batch\n",
    "            max_len = max(len(tokens) for tokens in batch_tokens)\n",
    "            padded_tokens = tf.constant([\n",
    "                tokens + [''] * (max_len - len(tokens)) for tokens in batch_tokens\n",
    "            ], dtype=tf.string)\n",
    "            sequence_lengths = tf.constant([len(tokens) for tokens in batch_tokens], dtype=tf.int32)\n",
    "\n",
    "            # Get ELMo outputs using tokens signature\n",
    "            outputs = elmo.signatures[\"tokens\"](tokens=padded_tokens, sequence_len=sequence_lengths)\n",
    "            embeddings = outputs[\"lstm_outputs2\"]  # last LSTM layer\n",
    "\n",
    "            # Apply pooling along token axis (axis=1)\n",
    "            batch_embeddings = []\n",
    "            for j in range(len(batch_texts)):\n",
    "                seq_len = sequence_lengths[j].numpy()\n",
    "                seq_embeddings = embeddings[j, :seq_len, :] # Select non-padded embeddings\n",
    "\n",
    "                if pooling == 'mean':\n",
    "                    doc_embedding = tf.reduce_mean(seq_embeddings, axis=0).numpy()\n",
    "                elif pooling == 'max':\n",
    "                    doc_embedding = tf.reduce_max(seq_embeddings, axis=0).numpy()\n",
    "                elif pooling == 'last':\n",
    "                    doc_embedding = seq_embeddings[-1, :].numpy()\n",
    "                else:\n",
    "                    raise ValueError(\"Invalid pooling method.\")\n",
    "                batch_embeddings.append(doc_embedding)\n",
    "            doc_embeddings.extend(batch_embeddings)\n",
    "\n",
    "        return np.array(doc_embeddings)\n",
    "\n",
    "    elif model == 'infersent':\n",
    "        # Assuming InferSent model is loaded and configured\n",
    "        model.build_vocab(texts, tokenize=True)\n",
    "        doc_embeddings = model.encode(texts, tokenize=True)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Model must be 'elmo' or 'infersent'.\")\n",
    "\n",
    "    return doc_embeddings"
   ],
   "id": "33132a1947be4b79"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def normalize_features(X, method='l2', eps=1e-10):\n",
    "    \"\"\"\n",
    "    Normalize feature matrix X using specified method.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): Input feature matrix of shape (n_samples, n_features).\n",
    "        method (str): Normalization method. One of:\n",
    "                      'identity'  -> no normalization\n",
    "                      'l2'        -> L2 normalization (unit length)\n",
    "                      'layernorm' -> normalize each vector by mean and std\n",
    "        eps (float): Small constant to avoid division by zero.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Normalized feature matrix.\n",
    "    \"\"\"\n",
    "    if method == 'identity':\n",
    "        return X\n",
    "    elif method == 'l2':\n",
    "        norms = np.linalg.norm(X, axis=1, keepdims=True)\n",
    "        return X / (norms + eps)\n",
    "    elif method == 'layernorm':\n",
    "        means = np.mean(X, axis=1, keepdims=True)\n",
    "        stds = np.std(X, axis=1, keepdims=True)\n",
    "        return (X - means) / (stds + eps)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid normalization method.\")"
   ],
   "id": "fa2a5a13f1a2398a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 2: Clustering\n",
    "In this step, we apply clustering algorithms on the normalized deep feature representations obtained in Step 1.\n",
    "We will use **K-means** as the primary clustering algorithm, but other methods like **Agglomerative Clustering** or **DBSCAN** can also be employed.\n",
    "The output of this step is a set of cluster assignments for each document, which will be used in the next step for explanation.\n"
   ],
   "id": "51cdfdc89bf99c4e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def cluster_features(X, n_clusters=10, random_state=42):\n",
    "    \"\"\"\n",
    "    Cluster feature matrix X using K-means.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): Input feature matrix of shape (n_samples, n_features).\n",
    "        n_clusters (int): Number of clusters.\n",
    "        random_state (int): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Cluster labels for each sample.\n",
    "        KMeans: Fitted KMeans model.\n",
    "    \"\"\"\n",
    "    kmeans = KMeans(\n",
    "        n_clusters=n_clusters,\n",
    "        random_state=random_state,\n",
    "        n_init=10,\n",
    "        max_iter=300\n",
    "    )\n",
    "    predicted_labels = kmeans.fit_predict(X)\n",
    "    return predicted_labels, kmeans"
   ],
   "id": "319020e65f6658fe"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 3: Evaluation Metrics\n",
    "\n",
    "To evaluate the clustering performance, we rely on three standard metrics used in the paper:\n",
    "\n",
    "- **Clustering Accuracy (ACC)**\n",
    "  Measures the best alignment between predicted clusters and ground-truth labels.\n",
    "  Since cluster IDs are arbitrary, the Hungarian algorithm is used to find the optimal mapping.\n",
    "\n",
    "- **Normalized Mutual Information (NMI)**\n",
    "  Measures the mutual dependence between predicted clusters and true labels.\n",
    "  Values range from 0 (no mutual information) to 1 (perfect correlation).\n",
    "\n",
    "- **Adjusted Rand Index (ARI)**\n",
    "  Measures the similarity between two assignments, adjusted for chance.\n",
    "  Values range from -1 to 1, where 1 indicates perfect agreement.\n",
    "\n",
    "These metrics provide complementary views:\n",
    "- **ACC** focuses on label alignment,\n",
    "- **NMI** evaluates information overlap,\n",
    "- **ARI** accounts for random chance.\n"
   ],
   "id": "8eb1d4c3aa45eb53"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def clustering_accuracy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate clustering accuracy using the Hungarian algorithm.\n",
    "\n",
    "    Args:\n",
    "        y_true (np.ndarray): Ground truth labels.\n",
    "        y_pred (np.ndarray): Predicted cluster labels.\n",
    "\n",
    "    Returns:\n",
    "        float: Clustering accuracy.\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true).astype(np.int64) # Ensure integer type\n",
    "    y_pred = np.asarray(y_pred).astype(np.int64)\n",
    "    assert y_pred.size == y_true.size # Ensure same size\n",
    "\n",
    "    D = max(y_pred.max(), y_true.max()) + 1 # Number of clusters\n",
    "    w = np.zeros((D, D), dtype=np.int64)\n",
    "    for i in range(y_pred.size):\n",
    "        w[y_pred[i], y_true[i]] += 1 # Build contingency matrix\n",
    "\n",
    "    # Hungarian algorithm to find optimal assignment\n",
    "    row_ind, col_ind = linear_sum_assignment(w.max() - w) # Maximize accuracy, linear_sum_assignment minimizes cost\n",
    "    mapping = dict(zip(row_ind, col_ind)) # Create mapping\n",
    "\n",
    "    # Map predicted labels to true labels\n",
    "    y_pred_mapped = np.array([mapping[label] for label in y_pred])\n",
    "    accuracy = np.mean(y_pred_mapped == y_true) # Calculate accuracy\n",
    "    return accuracy"
   ],
   "id": "2f3a5f2631bf5b91"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def clustering_nmi(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate Normalized Mutual Information (NMI) between true and predicted labels.\n",
    "\n",
    "    Args:\n",
    "        y_true (np.ndarray): Ground truth labels.\n",
    "        y_pred (np.ndarray): Predicted cluster labels.\n",
    "\n",
    "    Returns:\n",
    "        float: NMI score.\n",
    "    \"\"\"\n",
    "    return normalized_mutual_info_score(y_true, y_pred)"
   ],
   "id": "40814d33a771c057"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def clustering_ari(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate Adjusted Rand Index (ARI) between true and predicted labels.\n",
    "\n",
    "    Args:\n",
    "        y_true (np.ndarray): Ground truth labels.\n",
    "        y_pred (np.ndarray): Predicted cluster labels.\n",
    "\n",
    "    Returns:\n",
    "        float: ARI score.\n",
    "    \"\"\"\n",
    "    return adjusted_rand_score(y_true, y_pred)"
   ],
   "id": "e9412d89c1d2d2d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 4: Feature Visualization\n",
    "\n",
    "To qualitatively assess the quality of the extracted features, we can project the high-dimensional embeddings into a 2D space and visualize them.\n",
    "\n",
    "We use **t-SNE (t-distributed Stochastic Neighbor Embedding)**, which is a nonlinear dimensionality reduction technique that preserves local similarities between points.\n",
    "\n",
    "If the extracted deep features are meaningful, samples from the same class should form compact clusters in the 2D visualization, while different classes should be well separated.\n",
    "\n",
    "This visualization helps to confirm whether the embeddings from ELMo or InferSent provide more discriminative representations compared to traditional methods like tf-idf.\n"
   ],
   "id": "b5cebcc3486cab7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def visualize_features(X, labels, title='Feature Visualization with t-SNE', n_samples=1000, random_state=42):\n",
    "    \"\"\"\n",
    "    Visualize high-dimensional features using t-SNE.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): Input feature matrix of shape (n_samples, n_features).\n",
    "        labels (np.ndarray): Ground truth labels for coloring.\n",
    "        title (str): Title of the plot.\n",
    "        n_samples (int): Number of samples to visualize (for large datasets).\n",
    "        random_state (int): Random seed for reproducibility.\n",
    "\n",
    "    \"\"\"\n",
    "    # Randomly sample a subset for visualization (for large datasets)\n",
    "    if X.shape[0] > n_samples:\n",
    "        np.random.seed(random_state)\n",
    "        indices = np.random.choice(X.shape[0], n_samples, replace=False)\n",
    "        X_sampled = X[indices]\n",
    "        labels_sampled = np.array(labels)[indices]\n",
    "    else:\n",
    "        X_sampled = X\n",
    "        labels_sampled = labels\n",
    "\n",
    "    tsne = TSNE(n_components=2, perplexity=30, learning_rate=200,\n",
    "                n_iter=1000, random_state=random_state, init=\"pca\") # Hyperparameters from paper\n",
    "    X_2d = tsne.fit_transform(X_sampled)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    scatter = plt.scatter(X_2d[:, 0], X_2d[:, 1], c=labels_sampled, cmap='tab10', alpha=0.7)\n",
    "    plt.colorbar(scatter, ticks=np.unique(labels_sampled))\n",
    "    plt.title(title)\n",
    "    plt.xlabel('t-SNE Dimension 1')\n",
    "    plt.ylabel('t-SNE Dimension 2')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ],
   "id": "8f136fb8c9404c4f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 5: Dataset Loading\n",
    "\n",
    "In this section, we will prepare the benchmark datasets used in the paper:\n",
    "\n",
    "- **AG News** (4 classes, news articles by topic)\n",
    "- **DBpedia** (14 classes, ontology categories)\n",
    "- **Yahoo! Answers** (10 classes, question-answer categories)\n",
    "- **Reuters (R2 / R5 subsets)** (2 or 5 classes from the Reuters-21578 corpus)\n",
    "\n",
    "Since these datasets are relatively large, we will use either the full datasets or balanced subsets (as done in the paper, e.g., 1000 samples per class for AG News, DBpedia, and Yahoo).\n",
    "\n",
    "The datasets will be loaded, preprocessed (tokenization, lowercasing, optional stopword removal), and split into:\n",
    "- **texts**: the raw documents\n",
    "- **labels**: the ground-truth category of each document\n",
    "\n",
    "These will be the inputs to our feature extraction pipeline.\n"
   ],
   "id": "a1e0e4c54453d5c6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Basic text cleaning: lowercasing, removing special characters.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text.\n",
    "\n",
    "    Returns:\n",
    "        str: Cleaned text.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"<.*?>\", \" \", text)  # remove html\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip() # remove extra spaces\n",
    "    return text"
   ],
   "id": "43e0443c8bd5a40d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def load_subset(dataset, n_per_class=1000, classes=10, label_key='label'):\n",
    "    \"\"\"\n",
    "    Load a balanced subset of a dataset with specified number of samples per class.\n",
    "\n",
    "    Args:\n",
    "        dataset (Dataset): Huggingface dataset object.\n",
    "        n_per_class (int): Number of samples per class to load.\n",
    "        classes (int): Number of class labels to include.\n",
    "        label_key (str): Key in the dataset for class labels.\n",
    "\n",
    "    Returns:\n",
    "        texts (list of str): List of documents.\n",
    "        labels (list of int): Corresponding class labels.\n",
    "    \"\"\"\n",
    "    texts, labels = [], []\n",
    "    for label in range(classes):\n",
    "        samples = dataset.filter(lambda x: x[label_key] == label).select(range(n_per_class))\n",
    "        texts.extend([clean_text(item['text']) for item in samples])\n",
    "        if label_key == 'topic':  # Yahoo! Answers uses 'topic' as label key\n",
    "            texts.extend([clean_text(item['question_title'] + \" \" + item['best_answer']) for item in samples])\n",
    "        labels.extend(samples[label_key])\n",
    "    return texts, labels"
   ],
   "id": "c0abbea193b7ae33"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def load_dbpedia(n_per_class=1000):\n",
    "    \"\"\"\n",
    "    Load and preprocess DBpedia dataset.\n",
    "\n",
    "    Args:\n",
    "        n_per_class (int): Number of samples per class to load.\n",
    "\n",
    "    Returns:\n",
    "        texts (list of str): List of documents.\n",
    "        labels (list of int): Corresponding class labels.\n",
    "    \"\"\"\n",
    "    dataset = load_dataset('dbpedia_14', split='train')\n",
    "    return load_subset(dataset, n_per_class, classes=14, label_key='label')"
   ],
   "id": "c567d410470a76b1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def load_ag_news(n_per_class=1000):\n",
    "    \"\"\"\n",
    "    Load and preprocess AG News dataset.\n",
    "\n",
    "    Args:\n",
    "        n_per_class (int): Number of samples per class to load.\n",
    "\n",
    "    Returns:\n",
    "        texts (list of str): List of documents.\n",
    "        labels (list of int): Corresponding class labels.\n",
    "    \"\"\"\n",
    "    dataset = load_dataset('ag_news', split='train')\n",
    "    return load_subset(dataset, n_per_class, classes=4, label_key='label')"
   ],
   "id": "bcb39c8a3dc9cf96"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def load_yahoo(n_per_class=1000):\n",
    "    \"\"\"\n",
    "    Load and preprocess Yahoo! Answers dataset.\n",
    "\n",
    "    Args:\n",
    "        n_per_class (int): Number of samples per class to load.\n",
    "\n",
    "    Returns:\n",
    "        texts (list of str): List of documents.\n",
    "        labels (list of int): Corresponding class labels.\n",
    "    \"\"\"\n",
    "    dataset = load_dataset('yahoo_answers_topics', split='train')\n",
    "    return load_subset(dataset, n_per_class, classes=10, label_key='topic')"
   ],
   "id": "f6c86d14fb0e6f8a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def load_reuters(subset='R2'):\n",
    "    \"\"\"\n",
    "    Load and preprocess Reuters dataset (R2 or R5 subset).\n",
    "\n",
    "    Args:\n",
    "        subset (str): 'R2' for 2 classes, 'R5' for 5 classes.\n",
    "\n",
    "    Returns:\n",
    "        texts (list of str): List of documents.\n",
    "        labels (list of int): Corresponding class labels.\n",
    "    \"\"\"\n",
    "    dataset = load_dataset('reuters21578', 'ModApte', split='train')\n",
    "    if subset == 'R2':\n",
    "        # Binary classification: 'earn' vs 'acq'\n",
    "        texts, labels = [], []\n",
    "        for item in dataset:\n",
    "            if 'earn' in item['topics']:\n",
    "                texts.append(clean_text(item['text']))\n",
    "                labels.append(0)\n",
    "            elif 'acq' in item['topics']:\n",
    "                texts.append(clean_text(item['text']))\n",
    "                labels.append(1)\n",
    "        return texts, labels\n",
    "    elif subset == 'R5':\n",
    "        # 5 classes: 'earn', 'acq', 'crude', 'trade', 'money-fx'\n",
    "        class_map = {'earn': 0, 'acq': 1, 'crude': 2, 'trade': 3, 'money-fx': 4}\n",
    "        texts, labels = [], []\n",
    "        for item in dataset:\n",
    "            for topic in item['topics']:\n",
    "                if topic in class_map:\n",
    "                    texts.append(clean_text(item['text']))\n",
    "                    labels.append(class_map[topic])\n",
    "                    break\n",
    "        return texts, labels\n",
    "    else:\n",
    "        raise ValueError(\"Subset must be 'R2' or 'R5'.\")"
   ],
   "id": "90e22938b184c5b0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Step 6: Experiments\n",
    "\n",
    "In this section, we replicate the experimental setup from the paper *\"Deep Feature-Based Text Clustering and its Explanation\"*.\n",
    "\n",
    "The goal is to evaluate how different design choices affect clustering performance, namely:\n",
    "- **Embedding model**: ELMo (LM) or InferSent.\n",
    "- **Pooling strategy**: Mean, Max, or Last pooling over token embeddings.\n",
    "- **Normalization**: Identity (I), L2 norm (N), or LayerNorm (LN).\n",
    "- **Clustering algorithm**: K-means (KM).\n",
    "\n",
    "We report results on three benchmark datasets used in the paper:\n",
    "- **AG News** (4 classes, news articles)\n",
    "- **DBpedia** (14 classes, ontology categories)\n",
    "- **Yahoo Answers** (10 classes, QA topics)\n",
    "\n",
    "The evaluation is done using the metrics described earlier: **Clustering Accuracy (ACC)**, **Normalized Mutual Information (NMI)**, and **Adjusted Rand Index (ARI)**."
   ],
   "id": "b4959b42b7c56cd6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 6.1 Experiments with ELMo (LM)\n",
    "\n",
    "We first focus on the ELMo model (LM-based embeddings).\n",
    "For each dataset, we apply **mean pooling** over the last layer token representations, followed by different normalization strategies, and then cluster the resulting features using K-means (KM).\n",
    "\n",
    "### Experiment 1: LM + Mean + LN + KM on AG News\n",
    "Here we use **LayerNorm** as the normalization method on top of mean-pooled ELMo embeddings for AG News.\n",
    "\n",
    "### Experiment 2: LM + Mean + I + KM on DBpedia\n",
    "Here we use **Identity normalization** (no normalization) with mean-pooled embeddings for DBpedia.\n",
    "\n",
    "### Experiment 3: LM + Mean + N + KM on AG News, DBpedia, and Yahoo Answers\n",
    "Here we apply **L2 normalization** after mean pooling, and compare performance across three datasets: AG News, DBpedia, and Yahoo Answers.\n"
   ],
   "id": "a4eed174aebc7b13"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 1. Load dataset\n",
    "texts, labels = load_agnews(n_per_class=1000)\n",
    "print(\"Loaded AG News subset:\", len(texts))\n",
    "\n",
    "# 2. Extract features with ELMo + max pooling\n",
    "X = extract_features(texts, model='elmo', pooling='max')\n",
    "print(\"Feature matrix:\", X.shape)\n",
    "\n",
    "# 3. Apply Identity normalization\n",
    "X_norm = normalize_features(X, method=\"identity\")\n",
    "\n",
    "# 4. Cluster with K-means (k=4)\n",
    "y_pred, _ = cluster_features(X_norm, n_clusters=4)\n",
    "\n",
    "# 5. Evaluate with ACC, NMI, ARI\n",
    "acc = clustering_accuracy(labels, y_pred)\n",
    "nmi = clustering_nmi(labels, y_pred)\n",
    "ari = clustering_ari(labels, y_pred)\n",
    "\n",
    "print(f\"Results on AG News (ELMo + Max + Identity + KMeans):\")\n",
    "# metrics in %\n",
    "print(f\"ACC: {acc*100:.2f}%\")\n",
    "print(f\"NMI: {nmi*100:.2f}%\")\n",
    "print(f\"ARI: {ari*100:.2f}%\")"
   ],
   "id": "eb4a8a972b56e8d8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Experiment 1: LM + Mean + LN + KM on AG News\n",
    "texts, labels = load_ag_news(n_per_class=1000)\n",
    "\n",
    "X = extract_features(texts, model='elmo', pooling='mean')\n",
    "X_norm = normalize_features(X, method=\"layernorm\")\n",
    "y_pred, _ = cluster_features(X_norm, n_clusters=4)\n",
    "\n",
    "acc = clustering_accuracy(labels, y_pred)\n",
    "nmi = clustering_nmi(labels, y_pred)\n",
    "ari = clustering_ari(labels, y_pred)\n",
    "\n",
    "print(f\"Results on AG News (ELMo + Mean + LayerNorm + KMeans):\")\n",
    "print(f\"ACC: {acc*100:.2f}%\")\n",
    "print(f\"NMI: {nmi*100:.2f}%\")\n",
    "print(f\"ARI: {ari*100:.2f}%\")\n",
    "\n",
    "# Plot feature visualization\n",
    "visualize_features(X_norm, labels, title='AG News (ELMo + Mean + LayerNorm)')"
   ],
   "id": "83434dedd4fe525d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Experiment 2: LM + Mean + I + KM on DBpedia\n",
    "texts, labels = load_dbpedia(n_per_class=1000)\n",
    "\n",
    "X = extract_features(texts, model='elmo', pooling='mean')\n",
    "X_norm = normalize_features(X, method=\"identity\")\n",
    "y_pred , _ = cluster_features(X_norm, n_clusters=14)\n",
    "\n",
    "acc = clustering_accuracy(labels, y_pred)\n",
    "nmi = clustering_nmi(labels, y_pred)\n",
    "ari = clustering_ari(labels, y_pred)\n",
    "\n",
    "print(f\"Results on DBpedia (ELMo + Mean + Identity + KMeans):\")\n",
    "print(f\"ACC: {acc*100:.2f}%\")\n",
    "print(f\"NMI: {nmi*100:.2f}%\")\n",
    "print(f\"ARI: {ari*100:.2f}%\")\n",
    "\n",
    "# Plot feature visualization\n",
    "visualize_features(X_norm, labels, title='DBpedia (ELMo + Mean + Identity)')"
   ],
   "id": "de1a0a2abca8d315"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Experiment 3: LM + Mean + N + KM on AG News, DBpedia, and Yahoo Answers\n",
    "for dataset_name, load_func, n_clusters in [\n",
    "    ('AG News', load_ag_news, 4),\n",
    "    ('DBpedia', load_dbpedia, 14),\n",
    "    ('Yahoo Answers', load_yahoo, 10)\n",
    "]:\n",
    "    texts, labels = load_func(n_per_class=1000)\n",
    "    X = extract_features(texts, model='elmo', pooling='mean')\n",
    "    X_norm = normalize_features(X, method=\"l2\")\n",
    "    y_pred, _ = cluster_features(X_norm, n_clusters=n_clusters)\n",
    "\n",
    "    acc = clustering_accuracy(labels, y_pred)\n",
    "    nmi = clustering_nmi(labels, y_pred)\n",
    "    ari = clustering_ari(labels, y_pred)\n",
    "\n",
    "    print(f\"Results on {dataset_name} (ELMo + Mean + L2 + KMeans):\")\n",
    "    print(f\"ACC: {acc*100:.2f}%\")\n",
    "    print(f\"NMI: {nmi*100:.2f}%\")\n",
    "    print(f\"ARI: {ari*100:.2f}%\")\n",
    "\n",
    "    # Plot feature visualization\n",
    "    visualize_features(X_norm, labels, title=f'{dataset_name} (ELMo + Mean + L2)')"
   ],
   "id": "1aeac52efd2487b4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 6.2 Experiments with InferSent\n",
    "\n",
    "Next, we evaluate the performance of **InferSent**, a sentence embedding model trained on Natural Language Inference (NLI) data.\n",
    "Unlike ELMo, InferSent directly provides fixed-size sentence embeddings (4096 dimensions), which makes feature extraction straightforward.\n",
    "We experiment with two normalization strategies before applying K-means clustering.\n",
    "\n",
    "### Experiment 4: InferSent + LN + KM on DBpedia\n",
    "Here we apply **LayerNorm** to InferSent embeddings of DBpedia samples before clustering with K-means.\n",
    "\n",
    "### Experiment 5: InferSent + N + KM on AG News\n",
    "Here we apply **L2 normalization** to InferSent embeddings of AG News samples before clustering with K-means.\n"
   ],
   "id": "8396974b44f2eee6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Experiment 4: InferSent + LN + KM on DBpedia\n",
    "texts, labels = load_dbpedia(n_per_class=1000)\n",
    "\n",
    "X = extract_features(texts, model='infersent')\n",
    "X_norm = normalize_features(X, method=\"layernorm\")\n",
    "y_pred, _ = cluster_features(X_norm, n_clusters=14)\n",
    "\n",
    "acc = clustering_accuracy(labels, y_pred)\n",
    "nmi = clustering_nmi(labels, y_pred)\n",
    "ari = clustering_ari(labels, y_pred)\n",
    "\n",
    "print(f\"Results on DBpedia (InferSent + LayerNorm + KMeans):\")\n",
    "print(f\"ACC: {acc*100:.2f}%\")\n",
    "print(f\"NMI: {nmi*100:.2f}%\")\n",
    "print(f\"ARI: {ari*100:.2f}%\")\n",
    "\n",
    "# Plot feature visualization\n",
    "visualize_features(X_norm, labels, title='DBpedia (InferSent + LayerNorm)')"
   ],
   "id": "67391557d566e282"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Experiment 5: InferSent + N + KM on AG News\n",
    "texts, labels = load_ag_news(n_per_class=1000)\n",
    "\n",
    "X = extract_features(texts, model='infersent')\n",
    "X_norm = normalize_features(X, method=\"l2\")\n",
    "y_pred, _ = cluster_features(X_norm, n_clusters=14)\n",
    "\n",
    "acc = clustering_accuracy(labels, y_pred)\n",
    "nmi = clustering_nmi(labels, y_pred)\n",
    "ari = clustering_ari(labels, y_pred)\n",
    "\n",
    "print(f\"Results on AG News (InferSent + L2 + KMeans):\")\n",
    "print(f\"ACC: {acc*100:.2f}%\")\n",
    "print(f\"NMI: {nmi*100:.2f}%\")\n",
    "print(f\"ARI: {ari*100:.2f}%\")\n",
    "\n",
    "# Plot feature visualization\n",
    "visualize_features(X_norm, labels, title='AG News (InferSent + L2)')"
   ],
   "id": "74f71e9e936890a6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 7: Explainability – TCRE Model\n",
    "\n",
    "To interpret the discovered clusters, the paper proposes the **Text Clustering Result Explanation (TCRE)** model.\n",
    "The idea is to identify **indication words** that best characterize each cluster.\n",
    "\n",
    "The procedure (Algorithm 1 in the paper) works as follows:\n",
    "\n",
    "1. Convert each document into a **binary bag-of-words feature vector** (1 if the word appears, 0 otherwise).\n",
    "2. Filter out stop words and low-frequency words to reduce noise.\n",
    "3. Train a **logistic regression classifier** using the cluster assignments as pseudo-labels.\n",
    "4. Inspect the absolute values of the learned weights to find the **most important words** for each cluster.\n",
    "\n",
    "The output is, for each cluster, a ranked list of **indication words** that can be used to explain the cluster’s semantics.\n"
   ],
   "id": "e0bd9c25da0cc8d3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def tcre_explanation(corpus, cluster_labels, top_n=10, min_df=5, stop_words='english'):\n",
    "    \"\"\"\n",
    "    Text Clustering Result Explanation (TCRE):\n",
    "    Identify indication words for each cluster.\n",
    "\n",
    "    Args:\n",
    "        corpus (list of str): List of documents (raw text).\n",
    "        cluster_labels (array-like): Cluster assignments for each document.\n",
    "        top_n (int): Number of top indication words per cluster.\n",
    "        min_df (int): Minimum document frequency for a word to be kept.\n",
    "        stop_words (str or list): Stop words to remove.\n",
    "\n",
    "    Returns:\n",
    "        dict: {cluster_id: [indication words]}\n",
    "    \"\"\"\n",
    "    # 1. Convert documents to binary bag-of-words features\n",
    "    vectorizer = CountVectorizer(binary=True, stop_words=stop_words, min_df=min_df)\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    vocab = np.array(vectorizer.get_feature_names_out())\n",
    "\n",
    "    # 2. Train logistic regression with pseudo-labels\n",
    "    clf = LogisticRegression(max_iter=1000, multi_class='ovr')\n",
    "    clf.fit(X, cluster_labels)\n",
    "\n",
    "    # 3. For each cluster, get top absolute-weight words\n",
    "    ind_words = {}\n",
    "    weights = clf.coef_  # shape (n_clusters, n_features)\n",
    "\n",
    "    for cluster_id, w in enumerate(weights):\n",
    "        abs_w = np.abs(w)\n",
    "        top_idx = np.argsort(abs_w)[::-1][:top_n]\n",
    "        ind_words[cluster_id] = vocab[top_idx].tolist()\n",
    "\n",
    "    return ind_words"
   ],
   "id": "ffe980a91093c299"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "indication_words = tcre_explanation(texts, y_pred, top_n=10)\n",
    "\n",
    "for cluster, words in indication_words.items():\n",
    "    print(f\"Cluster {cluster}: {', '.join(words)}\")"
   ],
   "id": "3e2a618503d27b8f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
