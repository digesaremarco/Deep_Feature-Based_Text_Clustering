{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Deep Feature-Based Text Clustering and Its Explanation\n",
    "\n",
    "This notebook is a reproduction of the paper *\"Deep Feature-Based Text Clustering and its Explanation\"* by Guan et al. (IEEE TKDE, 2022).\n",
    "\n",
    "The paper addresses the limitations of traditional text clustering approaches, which are usually based on the bag-of-words representation and suffer from high dimensionality, sparsity, and lack of contextual/sequence information.\n",
    "\n",
    "The authors propose a novel framework called **Deep Feature-Based Text Clustering (DFTC)** that leverages pretrained deep text encoders (ELMo and InferSent) to generate contextualized sentence/document embeddings. These embeddings are then normalized and clustered using classical algorithms such as K-means.\n",
    "\n",
    "Additionally, the paper introduces the **Text Clustering Results Explanation (TCRE)** module, which applies a logistic regression model on bag-of-words features with pseudo-labels derived from clustering. This allows the extraction of *indication words* that explain the semantics of each cluster, providing interpretability and qualitative evaluation of the results.\n",
    "\n",
    "Experiments on multiple benchmark datasets (AG News, DBpedia, Yahoo! Answers, Reuters) demonstrate that the proposed framework outperforms traditional clustering methods (tf-idf+KMeans, LDA, GSDMM), deep clustering models (DEC, IDEC, STC), and even BERT in most cases. The combination of **deep semantic features + interpretability** makes DFTC an effective and transparent solution for unsupervised text clustering.\n"
   ],
   "id": "ecdac3a081b5a449"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T09:31:55.994857Z",
     "start_time": "2025-09-04T09:31:45.127684Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow_hub as hub\n",
    "import torch\n",
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import torch\n",
    "import numpy as np"
   ],
   "id": "de5c5109e5472ce1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Università\\LabDM\\.venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T09:33:29.748041Z",
     "start_time": "2025-09-04T09:31:58.750549Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load pre trained ELMo model\n",
    "\n",
    "elmo = hub.load(\"https://tfhub.dev/google/elmo/3\")\n",
    "print(elmo.signatures['default'].structured_outputs)"
   ],
   "id": "ddd0c688ce514154",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Università\\LabDM\\.venv\\Lib\\site-packages\\tensorflow_hub\\resolver.py:120: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Università\\LabDM\\.venv\\Lib\\site-packages\\tensorflow_hub\\resolver.py:120: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 3\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# Load pre trained ELMo model\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m elmo = \u001B[43mhub\u001B[49m\u001B[43m.\u001B[49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mhttps://tfhub.dev/google/elmo/3\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m      4\u001B[39m \u001B[38;5;28mprint\u001B[39m(elmo.signatures[\u001B[33m'\u001B[39m\u001B[33mdefault\u001B[39m\u001B[33m'\u001B[39m].structured_outputs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\Università\\LabDM\\.venv\\Lib\\site-packages\\tensorflow_hub\\module_v2.py:100\u001B[39m, in \u001B[36mload\u001B[39m\u001B[34m(handle, tags, options)\u001B[39m\n\u001B[32m     98\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(handle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[32m     99\u001B[39m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[33m\"\u001B[39m\u001B[33mExpected a string, got \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[33m\"\u001B[39m % handle)\n\u001B[32m--> \u001B[39m\u001B[32m100\u001B[39m module_path = \u001B[43mresolve\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    101\u001B[39m is_hub_module_v1 = tf.io.gfile.exists(_get_module_proto_path(module_path))\n\u001B[32m    102\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m tags \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m is_hub_module_v1:\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\Università\\LabDM\\.venv\\Lib\\site-packages\\tensorflow_hub\\module_v2.py:55\u001B[39m, in \u001B[36mresolve\u001B[39m\u001B[34m(handle)\u001B[39m\n\u001B[32m     31\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mresolve\u001B[39m(handle):\n\u001B[32m     32\u001B[39m \u001B[38;5;250m  \u001B[39m\u001B[33;03m\"\"\"Resolves a module handle into a path.\u001B[39;00m\n\u001B[32m     33\u001B[39m \n\u001B[32m     34\u001B[39m \u001B[33;03m  This function works both for plain TF2 SavedModels and the legacy TF1 Hub\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m     53\u001B[39m \u001B[33;03m    A string representing the Module path.\u001B[39;00m\n\u001B[32m     54\u001B[39m \u001B[33;03m  \"\"\"\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m55\u001B[39m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mregistry\u001B[49m\u001B[43m.\u001B[49m\u001B[43mresolver\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\Università\\LabDM\\.venv\\Lib\\site-packages\\tensorflow_hub\\registry.py:49\u001B[39m, in \u001B[36mMultiImplRegister.__call__\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m     47\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m impl \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mreversed\u001B[39m(\u001B[38;5;28mself\u001B[39m._impls):\n\u001B[32m     48\u001B[39m   \u001B[38;5;28;01mif\u001B[39;00m impl.is_supported(*args, **kwargs):\n\u001B[32m---> \u001B[39m\u001B[32m49\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mimpl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     50\u001B[39m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m     51\u001B[39m     fails.append(\u001B[38;5;28mtype\u001B[39m(impl).\u001B[34m__name__\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\Università\\LabDM\\.venv\\Lib\\site-packages\\tensorflow_hub\\compressed_module_resolver.py:81\u001B[39m, in \u001B[36mHttpCompressedFileResolver.__call__\u001B[39m\u001B[34m(self, handle)\u001B[39m\n\u001B[32m     77\u001B[39m   response = \u001B[38;5;28mself\u001B[39m._call_urlopen(request)\n\u001B[32m     78\u001B[39m   \u001B[38;5;28;01mreturn\u001B[39;00m resolver.DownloadManager(handle).download_and_uncompress(\n\u001B[32m     79\u001B[39m       response, tmp_dir)\n\u001B[32m---> \u001B[39m\u001B[32m81\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mresolver\u001B[49m\u001B[43m.\u001B[49m\u001B[43matomic_download\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdownload\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodule_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     82\u001B[39m \u001B[43m                                \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_lock_file_timeout_sec\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\Università\\LabDM\\.venv\\Lib\\site-packages\\tensorflow_hub\\resolver.py:421\u001B[39m, in \u001B[36matomic_download\u001B[39m\u001B[34m(handle, download_fn, module_dir, lock_file_timeout_sec)\u001B[39m\n\u001B[32m    419\u001B[39m logging.info(\u001B[33m\"\u001B[39m\u001B[33mDownloading TF-Hub Module \u001B[39m\u001B[33m'\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[33m'\u001B[39m\u001B[33m.\u001B[39m\u001B[33m\"\u001B[39m, handle)\n\u001B[32m    420\u001B[39m tf.compat.v1.gfile.MakeDirs(tmp_dir)\n\u001B[32m--> \u001B[39m\u001B[32m421\u001B[39m \u001B[43mdownload_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtmp_dir\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    422\u001B[39m \u001B[38;5;66;03m# Write module descriptor to capture information about which module was\u001B[39;00m\n\u001B[32m    423\u001B[39m \u001B[38;5;66;03m# downloaded by whom and when. The file stored at the same level as a\u001B[39;00m\n\u001B[32m    424\u001B[39m \u001B[38;5;66;03m# directory in order to keep the content of the 'model_dir' exactly as it\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    429\u001B[39m \u001B[38;5;66;03m# module caching protocol and no code in the TF-Hub library reads its\u001B[39;00m\n\u001B[32m    430\u001B[39m \u001B[38;5;66;03m# content.\u001B[39;00m\n\u001B[32m    431\u001B[39m _write_module_descriptor_file(handle, module_dir)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\Università\\LabDM\\.venv\\Lib\\site-packages\\tensorflow_hub\\compressed_module_resolver.py:78\u001B[39m, in \u001B[36mHttpCompressedFileResolver.__call__.<locals>.download\u001B[39m\u001B[34m(handle, tmp_dir)\u001B[39m\n\u001B[32m     75\u001B[39m request = urllib.request.Request(\n\u001B[32m     76\u001B[39m     \u001B[38;5;28mself\u001B[39m._append_compressed_format_query(handle))\n\u001B[32m     77\u001B[39m response = \u001B[38;5;28mself\u001B[39m._call_urlopen(request)\n\u001B[32m---> \u001B[39m\u001B[32m78\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mresolver\u001B[49m\u001B[43m.\u001B[49m\u001B[43mDownloadManager\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdownload_and_uncompress\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     79\u001B[39m \u001B[43m    \u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtmp_dir\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\Università\\LabDM\\.venv\\Lib\\site-packages\\tensorflow_hub\\resolver.py:192\u001B[39m, in \u001B[36mDownloadManager.download_and_uncompress\u001B[39m\u001B[34m(self, fileobj, dst_path)\u001B[39m\n\u001B[32m    182\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Streams the content for the 'fileobj' and stores the result in dst_path.\u001B[39;00m\n\u001B[32m    183\u001B[39m \n\u001B[32m    184\u001B[39m \u001B[33;03mArgs:\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    189\u001B[39m \u001B[33;03m  ValueError: Unknown object encountered inside the TAR file.\u001B[39;00m\n\u001B[32m    190\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    191\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m192\u001B[39m   \u001B[43mfile_utils\u001B[49m\u001B[43m.\u001B[49m\u001B[43mextract_tarfile_to_destination\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    193\u001B[39m \u001B[43m      \u001B[49m\u001B[43mfileobj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdst_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlog_function\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_log_progress\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    194\u001B[39m   total_size_str = tf_utils.bytes_to_readable_str(\n\u001B[32m    195\u001B[39m       \u001B[38;5;28mself\u001B[39m._total_bytes_downloaded, \u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m    196\u001B[39m   \u001B[38;5;28mself\u001B[39m._print_download_progress_msg(\n\u001B[32m    197\u001B[39m       \u001B[33m\"\u001B[39m\u001B[33mDownloaded \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[33m, Total size: \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[33m\"\u001B[39m % (\u001B[38;5;28mself\u001B[39m._url, total_size_str),\n\u001B[32m    198\u001B[39m       flush=\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\Università\\LabDM\\.venv\\Lib\\site-packages\\tensorflow_hub\\file_utils.py:52\u001B[39m, in \u001B[36mextract_tarfile_to_destination\u001B[39m\u001B[34m(fileobj, dst_path, log_function)\u001B[39m\n\u001B[32m     49\u001B[39m abs_target_path = merge_relative_path(dst_path, tarinfo.name)\n\u001B[32m     51\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m tarinfo.isfile():\n\u001B[32m---> \u001B[39m\u001B[32m52\u001B[39m   \u001B[43mextract_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtgz\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarinfo\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mabs_target_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlog_function\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlog_function\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     53\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m tarinfo.isdir():\n\u001B[32m     54\u001B[39m   tf.compat.v1.gfile.MakeDirs(abs_target_path)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\Università\\LabDM\\.venv\\Lib\\site-packages\\tensorflow_hub\\file_utils.py:35\u001B[39m, in \u001B[36mextract_file\u001B[39m\u001B[34m(tgz, tarinfo, dst_path, buffer_size, log_function)\u001B[39m\n\u001B[32m     33\u001B[39m dst = tf.compat.v1.gfile.GFile(dst_path, \u001B[33m\"\u001B[39m\u001B[33mwb\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     34\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[32m1\u001B[39m:\n\u001B[32m---> \u001B[39m\u001B[32m35\u001B[39m   buf = \u001B[43msrc\u001B[49m\u001B[43m.\u001B[49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbuffer_size\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     36\u001B[39m   \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m buf:\n\u001B[32m     37\u001B[39m     \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\Python311\\Lib\\tarfile.py:696\u001B[39m, in \u001B[36m_FileInFile.readinto\u001B[39m\u001B[34m(self, b)\u001B[39m\n\u001B[32m    695\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mreadinto\u001B[39m(\u001B[38;5;28mself\u001B[39m, b):\n\u001B[32m--> \u001B[39m\u001B[32m696\u001B[39m     buf = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    697\u001B[39m     b[:\u001B[38;5;28mlen\u001B[39m(buf)] = buf\n\u001B[32m    698\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(buf)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\Python311\\Lib\\tarfile.py:685\u001B[39m, in \u001B[36m_FileInFile.read\u001B[39m\u001B[34m(self, size)\u001B[39m\n\u001B[32m    683\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m data:\n\u001B[32m    684\u001B[39m     \u001B[38;5;28mself\u001B[39m.fileobj.seek(offset + (\u001B[38;5;28mself\u001B[39m.position - start))\n\u001B[32m--> \u001B[39m\u001B[32m685\u001B[39m     b = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mfileobj\u001B[49m\u001B[43m.\u001B[49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlength\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    686\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(b) != length:\n\u001B[32m    687\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m ReadError(\u001B[33m\"\u001B[39m\u001B[33munexpected end of data\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\Python311\\Lib\\tarfile.py:522\u001B[39m, in \u001B[36m_Stream.read\u001B[39m\u001B[34m(self, size)\u001B[39m\n\u001B[32m    520\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Return the next size number of bytes from the stream.\"\"\"\u001B[39;00m\n\u001B[32m    521\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m size \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m522\u001B[39m buf = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43msize\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    523\u001B[39m \u001B[38;5;28mself\u001B[39m.pos += \u001B[38;5;28mlen\u001B[39m(buf)\n\u001B[32m    524\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m buf\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\Python311\\Lib\\tarfile.py:540\u001B[39m, in \u001B[36m_Stream._read\u001B[39m\u001B[34m(self, size)\u001B[39m\n\u001B[32m    538\u001B[39m     \u001B[38;5;28mself\u001B[39m.buf = \u001B[33mb\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    539\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m540\u001B[39m     buf = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mfileobj\u001B[49m\u001B[43m.\u001B[49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mbufsize\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    541\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m buf:\n\u001B[32m    542\u001B[39m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\Python311\\Lib\\http\\client.py:465\u001B[39m, in \u001B[36mHTTPResponse.read\u001B[39m\u001B[34m(self, amt)\u001B[39m\n\u001B[32m    462\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.length \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m amt > \u001B[38;5;28mself\u001B[39m.length:\n\u001B[32m    463\u001B[39m     \u001B[38;5;66;03m# clip the read to the \"end of response\"\u001B[39;00m\n\u001B[32m    464\u001B[39m     amt = \u001B[38;5;28mself\u001B[39m.length\n\u001B[32m--> \u001B[39m\u001B[32m465\u001B[39m s = \u001B[38;5;28mself\u001B[39m.fp.read(amt)\n\u001B[32m    466\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m s \u001B[38;5;129;01mand\u001B[39;00m amt:\n\u001B[32m    467\u001B[39m     \u001B[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001B[39;00m\n\u001B[32m    468\u001B[39m     \u001B[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001B[39;00m\n\u001B[32m    469\u001B[39m     \u001B[38;5;28mself\u001B[39m._close_conn()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\Python311\\Lib\\socket.py:705\u001B[39m, in \u001B[36mSocketIO.readinto\u001B[39m\u001B[34m(self, b)\u001B[39m\n\u001B[32m    703\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[32m    704\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m705\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_sock\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrecv_into\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    706\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m timeout:\n\u001B[32m    707\u001B[39m         \u001B[38;5;28mself\u001B[39m._timeout_occurred = \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\Python311\\Lib\\ssl.py:1278\u001B[39m, in \u001B[36mSSLSocket.recv_into\u001B[39m\u001B[34m(self, buffer, nbytes, flags)\u001B[39m\n\u001B[32m   1274\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m flags != \u001B[32m0\u001B[39m:\n\u001B[32m   1275\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m   1276\u001B[39m           \u001B[33m\"\u001B[39m\u001B[33mnon-zero flags not allowed in calls to recv_into() on \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[33m\"\u001B[39m %\n\u001B[32m   1277\u001B[39m           \u001B[38;5;28mself\u001B[39m.\u001B[34m__class__\u001B[39m)\n\u001B[32m-> \u001B[39m\u001B[32m1278\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnbytes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbuffer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1279\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1280\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\Python311\\Lib\\ssl.py:1134\u001B[39m, in \u001B[36mSSLSocket.read\u001B[39m\u001B[34m(self, len, buffer)\u001B[39m\n\u001B[32m   1132\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m   1133\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m buffer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1134\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_sslobj\u001B[49m\u001B[43m.\u001B[49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbuffer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1135\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1136\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._sslobj.read(\u001B[38;5;28mlen\u001B[39m)\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Clone git repository\n",
    "!git clone https://github.com/facebookresearch/InferSent.git\n",
    "\n",
    "# Open the InferSent directory\n",
    "%cd InferSent\n",
    "\n",
    "# Install dependencies\n",
    "!pip install torch torchvision nltk\n",
    "\n",
    "# Download the pre-trained InferSent model\n",
    "!wget https://dl.fbaipublicfiles.com/infersent/infersent2.pkl\n",
    "\n",
    "# Download word embeddings (GloVe)\n",
    "!wget http://nlp.stanford.edu/data/glove.840B.300d.zip\n",
    "!unzip -q glove.840B.300d.zip"
   ],
   "id": "23223b73799c5530"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "from models import InferSent\n",
    "\n",
    "# Model parameters\n",
    "MODEL_PATH = 'infersent2.pkl'\n",
    "params_model = {\n",
    "    'bsize': 64,\n",
    "    'word_emb_dim': 300,\n",
    "    'enc_lstm_dim': 2048,\n",
    "    'pool_type': 'max',\n",
    "    'dpout_model': 0.0,\n",
    "    'version': 2\n",
    "}\n",
    "model = InferSent(params_model)\n",
    "model.load_state_dict(torch.load(MODEL_PATH))\n",
    "\n",
    "# GloVe embeddings path\n",
    "W2V_PATH = 'glove.840B.300d.txt'\n",
    "model.set_w2v_path(W2V_PATH)\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab') # for italian\n",
    "\n",
    "# Build vocabulary and encode sentences\n",
    "sentences = [\n",
    "    \"Ciao, sto usando InferSent su Colab.\",\n",
    "    \"Questo modello trasforma frasi in vettori.\"\n",
    "]\n",
    "model.build_vocab(sentences, tokenize=True)\n",
    "embeddings = model.encode(sentences, tokenize=True)\n",
    "\n",
    "print(embeddings.shape)   # (2, 4096)\n",
    "print(embeddings[0][:10]) # Show first 10 dimensions of the first sentence embedding\n"
   ],
   "id": "2af5d1022de38708"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 1: Feature Construction\n",
    "\n",
    "In this step, we transform the input documents into **deep feature representations** using two pretrained models: **ELMo** and **InferSent**.\n",
    "\n",
    "- **ELMo (Language Model based on BiLSTM)**\n",
    "  Provides contextualized word embeddings. To obtain a fixed-size vector for a document, we apply pooling operations over token-level embeddings (e.g., mean-pooling, max-pooling).\n",
    "\n",
    "- **InferSent (Supervised NLI sentence encoder)**\n",
    "  Produces high-quality sentence embeddings using a BiLSTM + max-pooling architecture. For documents with multiple sentences, we compute the average of sentence embeddings.\n",
    "\n",
    "The result of this step is a matrix **X** of shape `(n_docs, d)`, where `d = 1024` for ELMo or `d = 4096` for InferSent.\n",
    "These vectors will later be normalized and clustered (e.g., with K-means).\n",
    "\n",
    "---\n",
    "\n",
    "### DFTC framework overview\n",
    "\n",
    "Below is the overall architecture of the proposed framework from the paper:\n",
    "\n",
    "![DFTC Framework](DFTC_framework.png)\n",
    "\n",
    "*Figure: Deep Feature-Based Text Clustering (DFTC) framework. First, pretrained encoders generate document embeddings. Then, features are normalized and clustered. Finally, the TCRE module explains the clusters by identifying indication words.*\n"
   ],
   "id": "4d5c93c605e3d876"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "33132a1947be4b79"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
