{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Deep Feature-Based Text Clustering and Its Explanation\n",
    "\n",
    "This notebook is a reproduction of the paper *\"Deep Feature-Based Text Clustering and its Explanation\"* by Guan et al. (IEEE TKDE, 2022).\n",
    "\n",
    "The paper addresses the limitations of traditional text clustering approaches, which are usually based on the bag-of-words representation and suffer from high dimensionality, sparsity, and lack of contextual/sequence information.\n",
    "\n",
    "The authors propose a novel framework called **Deep Feature-Based Text Clustering (DFTC)** that leverages pretrained deep text encoders (ELMo and InferSent) to generate contextualized sentence/document embeddings. These embeddings are then normalized and clustered using classical algorithms such as K-means.\n",
    "\n",
    "Additionally, the paper introduces the **Text Clustering Results Explanation (TCRE)** module, which applies a logistic regression model on bag-of-words features with pseudo-labels derived from clustering. This allows the extraction of *indication words* that explain the semantics of each cluster, providing interpretability and qualitative evaluation of the results.\n",
    "\n",
    "Experiments on multiple benchmark datasets (AG News, DBpedia, Yahoo! Answers, Reuters) demonstrate that the proposed framework outperforms traditional clustering methods (tf-idf+KMeans, LDA, GSDMM), deep clustering models (DEC, IDEC, STC), and even BERT in most cases. The combination of **deep semantic features + interpretability** makes DFTC an effective and transparent solution for unsupervised text clustering.\n"
   ],
   "id": "ecdac3a081b5a449"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T09:31:55.994857Z",
     "start_time": "2025-09-04T09:31:45.127684Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import torch\n",
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab') # for italian\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE"
   ],
   "id": "de5c5109e5472ce1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Università\\LabDM\\.venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T09:33:29.748041Z",
     "start_time": "2025-09-04T09:31:58.750549Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load pre trained ELMo model\n",
    "\n",
    "elmo = hub.load(\"https://tfhub.dev/google/elmo/3\")\n",
    "print(elmo.signatures['default'].structured_outputs)"
   ],
   "id": "ddd0c688ce514154",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Università\\LabDM\\.venv\\Lib\\site-packages\\tensorflow_hub\\resolver.py:120: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Università\\LabDM\\.venv\\Lib\\site-packages\\tensorflow_hub\\resolver.py:120: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 3\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# Load pre trained ELMo model\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m elmo = \u001B[43mhub\u001B[49m\u001B[43m.\u001B[49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mhttps://tfhub.dev/google/elmo/3\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m      4\u001B[39m \u001B[38;5;28mprint\u001B[39m(elmo.signatures[\u001B[33m'\u001B[39m\u001B[33mdefault\u001B[39m\u001B[33m'\u001B[39m].structured_outputs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\Università\\LabDM\\.venv\\Lib\\site-packages\\tensorflow_hub\\module_v2.py:100\u001B[39m, in \u001B[36mload\u001B[39m\u001B[34m(handle, tags, options)\u001B[39m\n\u001B[32m     98\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(handle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[32m     99\u001B[39m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[33m\"\u001B[39m\u001B[33mExpected a string, got \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[33m\"\u001B[39m % handle)\n\u001B[32m--> \u001B[39m\u001B[32m100\u001B[39m module_path = \u001B[43mresolve\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    101\u001B[39m is_hub_module_v1 = tf.io.gfile.exists(_get_module_proto_path(module_path))\n\u001B[32m    102\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m tags \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m is_hub_module_v1:\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\Università\\LabDM\\.venv\\Lib\\site-packages\\tensorflow_hub\\module_v2.py:55\u001B[39m, in \u001B[36mresolve\u001B[39m\u001B[34m(handle)\u001B[39m\n\u001B[32m     31\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mresolve\u001B[39m(handle):\n\u001B[32m     32\u001B[39m \u001B[38;5;250m  \u001B[39m\u001B[33;03m\"\"\"Resolves a module handle into a path.\u001B[39;00m\n\u001B[32m     33\u001B[39m \n\u001B[32m     34\u001B[39m \u001B[33;03m  This function works both for plain TF2 SavedModels and the legacy TF1 Hub\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m     53\u001B[39m \u001B[33;03m    A string representing the Module path.\u001B[39;00m\n\u001B[32m     54\u001B[39m \u001B[33;03m  \"\"\"\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m55\u001B[39m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mregistry\u001B[49m\u001B[43m.\u001B[49m\u001B[43mresolver\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\Università\\LabDM\\.venv\\Lib\\site-packages\\tensorflow_hub\\registry.py:49\u001B[39m, in \u001B[36mMultiImplRegister.__call__\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m     47\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m impl \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mreversed\u001B[39m(\u001B[38;5;28mself\u001B[39m._impls):\n\u001B[32m     48\u001B[39m   \u001B[38;5;28;01mif\u001B[39;00m impl.is_supported(*args, **kwargs):\n\u001B[32m---> \u001B[39m\u001B[32m49\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mimpl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     50\u001B[39m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m     51\u001B[39m     fails.append(\u001B[38;5;28mtype\u001B[39m(impl).\u001B[34m__name__\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\Università\\LabDM\\.venv\\Lib\\site-packages\\tensorflow_hub\\compressed_module_resolver.py:81\u001B[39m, in \u001B[36mHttpCompressedFileResolver.__call__\u001B[39m\u001B[34m(self, handle)\u001B[39m\n\u001B[32m     77\u001B[39m   response = \u001B[38;5;28mself\u001B[39m._call_urlopen(request)\n\u001B[32m     78\u001B[39m   \u001B[38;5;28;01mreturn\u001B[39;00m resolver.DownloadManager(handle).download_and_uncompress(\n\u001B[32m     79\u001B[39m       response, tmp_dir)\n\u001B[32m---> \u001B[39m\u001B[32m81\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mresolver\u001B[49m\u001B[43m.\u001B[49m\u001B[43matomic_download\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdownload\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodule_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     82\u001B[39m \u001B[43m                                \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_lock_file_timeout_sec\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\Università\\LabDM\\.venv\\Lib\\site-packages\\tensorflow_hub\\resolver.py:421\u001B[39m, in \u001B[36matomic_download\u001B[39m\u001B[34m(handle, download_fn, module_dir, lock_file_timeout_sec)\u001B[39m\n\u001B[32m    419\u001B[39m logging.info(\u001B[33m\"\u001B[39m\u001B[33mDownloading TF-Hub Module \u001B[39m\u001B[33m'\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[33m'\u001B[39m\u001B[33m.\u001B[39m\u001B[33m\"\u001B[39m, handle)\n\u001B[32m    420\u001B[39m tf.compat.v1.gfile.MakeDirs(tmp_dir)\n\u001B[32m--> \u001B[39m\u001B[32m421\u001B[39m \u001B[43mdownload_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtmp_dir\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    422\u001B[39m \u001B[38;5;66;03m# Write module descriptor to capture information about which module was\u001B[39;00m\n\u001B[32m    423\u001B[39m \u001B[38;5;66;03m# downloaded by whom and when. The file stored at the same level as a\u001B[39;00m\n\u001B[32m    424\u001B[39m \u001B[38;5;66;03m# directory in order to keep the content of the 'model_dir' exactly as it\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    429\u001B[39m \u001B[38;5;66;03m# module caching protocol and no code in the TF-Hub library reads its\u001B[39;00m\n\u001B[32m    430\u001B[39m \u001B[38;5;66;03m# content.\u001B[39;00m\n\u001B[32m    431\u001B[39m _write_module_descriptor_file(handle, module_dir)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\Università\\LabDM\\.venv\\Lib\\site-packages\\tensorflow_hub\\compressed_module_resolver.py:78\u001B[39m, in \u001B[36mHttpCompressedFileResolver.__call__.<locals>.download\u001B[39m\u001B[34m(handle, tmp_dir)\u001B[39m\n\u001B[32m     75\u001B[39m request = urllib.request.Request(\n\u001B[32m     76\u001B[39m     \u001B[38;5;28mself\u001B[39m._append_compressed_format_query(handle))\n\u001B[32m     77\u001B[39m response = \u001B[38;5;28mself\u001B[39m._call_urlopen(request)\n\u001B[32m---> \u001B[39m\u001B[32m78\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mresolver\u001B[49m\u001B[43m.\u001B[49m\u001B[43mDownloadManager\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdownload_and_uncompress\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     79\u001B[39m \u001B[43m    \u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtmp_dir\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\Università\\LabDM\\.venv\\Lib\\site-packages\\tensorflow_hub\\resolver.py:192\u001B[39m, in \u001B[36mDownloadManager.download_and_uncompress\u001B[39m\u001B[34m(self, fileobj, dst_path)\u001B[39m\n\u001B[32m    182\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Streams the content for the 'fileobj' and stores the result in dst_path.\u001B[39;00m\n\u001B[32m    183\u001B[39m \n\u001B[32m    184\u001B[39m \u001B[33;03mArgs:\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    189\u001B[39m \u001B[33;03m  ValueError: Unknown object encountered inside the TAR file.\u001B[39;00m\n\u001B[32m    190\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    191\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m192\u001B[39m   \u001B[43mfile_utils\u001B[49m\u001B[43m.\u001B[49m\u001B[43mextract_tarfile_to_destination\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    193\u001B[39m \u001B[43m      \u001B[49m\u001B[43mfileobj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdst_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlog_function\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_log_progress\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    194\u001B[39m   total_size_str = tf_utils.bytes_to_readable_str(\n\u001B[32m    195\u001B[39m       \u001B[38;5;28mself\u001B[39m._total_bytes_downloaded, \u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m    196\u001B[39m   \u001B[38;5;28mself\u001B[39m._print_download_progress_msg(\n\u001B[32m    197\u001B[39m       \u001B[33m\"\u001B[39m\u001B[33mDownloaded \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[33m, Total size: \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[33m\"\u001B[39m % (\u001B[38;5;28mself\u001B[39m._url, total_size_str),\n\u001B[32m    198\u001B[39m       flush=\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\Università\\LabDM\\.venv\\Lib\\site-packages\\tensorflow_hub\\file_utils.py:52\u001B[39m, in \u001B[36mextract_tarfile_to_destination\u001B[39m\u001B[34m(fileobj, dst_path, log_function)\u001B[39m\n\u001B[32m     49\u001B[39m abs_target_path = merge_relative_path(dst_path, tarinfo.name)\n\u001B[32m     51\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m tarinfo.isfile():\n\u001B[32m---> \u001B[39m\u001B[32m52\u001B[39m   \u001B[43mextract_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtgz\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarinfo\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mabs_target_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlog_function\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlog_function\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     53\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m tarinfo.isdir():\n\u001B[32m     54\u001B[39m   tf.compat.v1.gfile.MakeDirs(abs_target_path)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\Università\\LabDM\\.venv\\Lib\\site-packages\\tensorflow_hub\\file_utils.py:35\u001B[39m, in \u001B[36mextract_file\u001B[39m\u001B[34m(tgz, tarinfo, dst_path, buffer_size, log_function)\u001B[39m\n\u001B[32m     33\u001B[39m dst = tf.compat.v1.gfile.GFile(dst_path, \u001B[33m\"\u001B[39m\u001B[33mwb\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     34\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[32m1\u001B[39m:\n\u001B[32m---> \u001B[39m\u001B[32m35\u001B[39m   buf = \u001B[43msrc\u001B[49m\u001B[43m.\u001B[49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbuffer_size\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     36\u001B[39m   \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m buf:\n\u001B[32m     37\u001B[39m     \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\Python311\\Lib\\tarfile.py:696\u001B[39m, in \u001B[36m_FileInFile.readinto\u001B[39m\u001B[34m(self, b)\u001B[39m\n\u001B[32m    695\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mreadinto\u001B[39m(\u001B[38;5;28mself\u001B[39m, b):\n\u001B[32m--> \u001B[39m\u001B[32m696\u001B[39m     buf = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    697\u001B[39m     b[:\u001B[38;5;28mlen\u001B[39m(buf)] = buf\n\u001B[32m    698\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(buf)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\Python311\\Lib\\tarfile.py:685\u001B[39m, in \u001B[36m_FileInFile.read\u001B[39m\u001B[34m(self, size)\u001B[39m\n\u001B[32m    683\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m data:\n\u001B[32m    684\u001B[39m     \u001B[38;5;28mself\u001B[39m.fileobj.seek(offset + (\u001B[38;5;28mself\u001B[39m.position - start))\n\u001B[32m--> \u001B[39m\u001B[32m685\u001B[39m     b = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mfileobj\u001B[49m\u001B[43m.\u001B[49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlength\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    686\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(b) != length:\n\u001B[32m    687\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m ReadError(\u001B[33m\"\u001B[39m\u001B[33munexpected end of data\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\Python311\\Lib\\tarfile.py:522\u001B[39m, in \u001B[36m_Stream.read\u001B[39m\u001B[34m(self, size)\u001B[39m\n\u001B[32m    520\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Return the next size number of bytes from the stream.\"\"\"\u001B[39;00m\n\u001B[32m    521\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m size \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m522\u001B[39m buf = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43msize\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    523\u001B[39m \u001B[38;5;28mself\u001B[39m.pos += \u001B[38;5;28mlen\u001B[39m(buf)\n\u001B[32m    524\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m buf\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\Python311\\Lib\\tarfile.py:540\u001B[39m, in \u001B[36m_Stream._read\u001B[39m\u001B[34m(self, size)\u001B[39m\n\u001B[32m    538\u001B[39m     \u001B[38;5;28mself\u001B[39m.buf = \u001B[33mb\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    539\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m540\u001B[39m     buf = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mfileobj\u001B[49m\u001B[43m.\u001B[49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mbufsize\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    541\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m buf:\n\u001B[32m    542\u001B[39m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\Python311\\Lib\\http\\client.py:465\u001B[39m, in \u001B[36mHTTPResponse.read\u001B[39m\u001B[34m(self, amt)\u001B[39m\n\u001B[32m    462\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.length \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m amt > \u001B[38;5;28mself\u001B[39m.length:\n\u001B[32m    463\u001B[39m     \u001B[38;5;66;03m# clip the read to the \"end of response\"\u001B[39;00m\n\u001B[32m    464\u001B[39m     amt = \u001B[38;5;28mself\u001B[39m.length\n\u001B[32m--> \u001B[39m\u001B[32m465\u001B[39m s = \u001B[38;5;28mself\u001B[39m.fp.read(amt)\n\u001B[32m    466\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m s \u001B[38;5;129;01mand\u001B[39;00m amt:\n\u001B[32m    467\u001B[39m     \u001B[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001B[39;00m\n\u001B[32m    468\u001B[39m     \u001B[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001B[39;00m\n\u001B[32m    469\u001B[39m     \u001B[38;5;28mself\u001B[39m._close_conn()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\Python311\\Lib\\socket.py:705\u001B[39m, in \u001B[36mSocketIO.readinto\u001B[39m\u001B[34m(self, b)\u001B[39m\n\u001B[32m    703\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[32m    704\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m705\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_sock\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrecv_into\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    706\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m timeout:\n\u001B[32m    707\u001B[39m         \u001B[38;5;28mself\u001B[39m._timeout_occurred = \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\Python311\\Lib\\ssl.py:1278\u001B[39m, in \u001B[36mSSLSocket.recv_into\u001B[39m\u001B[34m(self, buffer, nbytes, flags)\u001B[39m\n\u001B[32m   1274\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m flags != \u001B[32m0\u001B[39m:\n\u001B[32m   1275\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m   1276\u001B[39m           \u001B[33m\"\u001B[39m\u001B[33mnon-zero flags not allowed in calls to recv_into() on \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[33m\"\u001B[39m %\n\u001B[32m   1277\u001B[39m           \u001B[38;5;28mself\u001B[39m.\u001B[34m__class__\u001B[39m)\n\u001B[32m-> \u001B[39m\u001B[32m1278\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnbytes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbuffer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1279\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1280\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\Python311\\Lib\\ssl.py:1134\u001B[39m, in \u001B[36mSSLSocket.read\u001B[39m\u001B[34m(self, len, buffer)\u001B[39m\n\u001B[32m   1132\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m   1133\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m buffer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1134\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_sslobj\u001B[49m\u001B[43m.\u001B[49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbuffer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1135\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1136\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._sslobj.read(\u001B[38;5;28mlen\u001B[39m)\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Clone git repository\n",
    "!git clone https://github.com/facebookresearch/InferSent.git\n",
    "\n",
    "# Open the InferSent directory\n",
    "%cd InferSent\n",
    "\n",
    "# Install dependencies\n",
    "!pip install torch torchvision nltk\n",
    "\n",
    "# Download the pre-trained InferSent model\n",
    "!wget https://dl.fbaipublicfiles.com/infersent/infersent2.pkl\n",
    "\n",
    "# Download word embeddings (GloVe)\n",
    "!wget http://nlp.stanford.edu/data/glove.840B.300d.zip\n",
    "!unzip -q glove.840B.300d.zip"
   ],
   "id": "23223b73799c5530"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from models import InferSent\n",
    "\n",
    "# Model parameters\n",
    "MODEL_PATH = 'infersent2.pkl'\n",
    "params_model = {\n",
    "    'bsize': 64,\n",
    "    'word_emb_dim': 300,\n",
    "    'enc_lstm_dim': 2048,\n",
    "    'pool_type': 'max',\n",
    "    'dpout_model': 0.0,\n",
    "    'version': 2\n",
    "}\n",
    "infersent = InferSent(params_model)\n",
    "infersent.load_state_dict(torch.load(MODEL_PATH))\n",
    "\n",
    "# GloVe embeddings path\n",
    "W2V_PATH = 'glove.840B.300d.txt'\n",
    "infersent.set_w2v_path(W2V_PATH)"
   ],
   "id": "2af5d1022de38708"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 1: Feature Construction\n",
    "\n",
    "In this step, we transform the input documents into **deep feature representations** using two pretrained models: **ELMo** and **InferSent**.\n",
    "\n",
    "- **ELMo (Language Model based on BiLSTM)**\n",
    "  Provides contextualized word embeddings. To obtain a fixed-size vector for a document, we apply pooling operations over token-level embeddings (e.g., mean-pooling, max-pooling).\n",
    "\n",
    "- **InferSent (Supervised NLI sentence encoder)**\n",
    "  Produces high-quality sentence embeddings using a BiLSTM + max-pooling architecture. For documents with multiple sentences, we compute the average of sentence embeddings.\n",
    "\n",
    "The result of this step is a matrix **X** of shape `(n_docs, d)`, where `d = 1024` for ELMo or `d = 4096` for InferSent.\n",
    "These vectors will later be normalized and clustered (e.g., with K-means).\n",
    "\n",
    "---\n",
    "\n",
    "### DFTC framework overview\n",
    "\n",
    "Below is the overall architecture of the proposed framework from the paper:\n",
    "\n",
    "![DFTC Framework](DFTC_framework.png)\n",
    "\n",
    "*Figure: Deep Feature-Based Text Clustering (DFTC) framework. First, pretrained encoders generate document embeddings. Then, features are normalized and clustered. Finally, the TCRE module explains the clusters by identifying indication words.*\n"
   ],
   "id": "4d5c93c605e3d876"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def extract_features(texts, model='elmo', pooling='mean'):\n",
    "    \"\"\"\n",
    "    Extract deep feature representations for a list of texts using ELMo or InferSent.\n",
    "\n",
    "    Args:\n",
    "        texts (list of str): List of input documents.\n",
    "        model (str): 'elmo' or 'infersent' to choose the embedding model.\n",
    "        pooling (str): Pooling strategy for ELMo ('mean', 'max', 'concat').\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Matrix of shape (n_docs, d) with document embeddings.\n",
    "    \"\"\"\n",
    "    if model == 'elmo':\n",
    "        doc_embeddings = []\n",
    "        for text in texts:\n",
    "            # Tokenize text\n",
    "            tokenized_text = word_tokenize(text)\n",
    "\n",
    "            # Convert list of tokens to a 1D tensor of strings\n",
    "            text_tensor = tf.constant(tokenized_text, dtype=tf.string)\n",
    "\n",
    "            # Get ELMo embeddings for the single text\n",
    "            embeddings = elmo.signatures['default'](text_tensor)['elmo']\n",
    "\n",
    "            # Apply pooling\n",
    "            if pooling == 'mean':\n",
    "                doc_embedding = tf.reduce_mean(embeddings, axis=0).numpy()\n",
    "            elif pooling == 'max':\n",
    "                doc_embedding = tf.reduce_max(embeddings, axis=0).numpy()\n",
    "            elif pooling == 'last':\n",
    "                doc_embedding = embeddings[-1, :].numpy()\n",
    "            else:\n",
    "                raise ValueError(\"Invalid pooling method.\")\n",
    "\n",
    "            doc_embeddings.append(doc_embedding)\n",
    "\n",
    "        return np.array(doc_embeddings)\n",
    "\n",
    "    elif model == 'infersent':\n",
    "        # Build vocabulary and encode texts\n",
    "        infersent.build_vocab(texts, tokenize=True)\n",
    "        doc_embeddings = infersent.encode(texts, tokenize=True)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Model must be 'elmo' or 'infersent'.\")\n",
    "\n",
    "    return doc_embeddings"
   ],
   "id": "33132a1947be4b79"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def normalize_features(X, method='l2', eps=1e-10):\n",
    "    \"\"\"\n",
    "    Normalize feature matrix X using specified method.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): Input feature matrix of shape (n_samples, n_features).\n",
    "        method (str): Normalization method. One of:\n",
    "                      'identity'  -> no normalization\n",
    "                      'l2'        -> L2 normalization (unit length)\n",
    "                      'layernorm' -> normalize each vector by mean and std\n",
    "        eps (float): Small constant to avoid division by zero.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Normalized feature matrix.\n",
    "    \"\"\"\n",
    "    if method == 'identity':\n",
    "        return X\n",
    "    elif method == 'l2':\n",
    "        norms = np.linalg.norm(X, axis=1, keepdims=True)\n",
    "        return X / (norms + eps)\n",
    "    elif method == 'layernorm':\n",
    "        means = np.mean(X, axis=1, keepdims=True)\n",
    "        stds = np.std(X, axis=1, keepdims=True)\n",
    "        return (X - means) / (stds + eps)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid normalization method.\")"
   ],
   "id": "fa2a5a13f1a2398a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 2: Clustering\n",
    "In this step, we apply clustering algorithms on the normalized deep feature representations obtained in Step 1.\n",
    "We will use **K-means** as the primary clustering algorithm, but other methods like **Agglomerative Clustering** or **DBSCAN** can also be employed.\n",
    "The output of this step is a set of cluster assignments for each document, which will be used in the next step for explanation.\n"
   ],
   "id": "51cdfdc89bf99c4e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def cluster_features(X, n_clusters=10, random_state=42):\n",
    "    \"\"\"\n",
    "    Cluster feature matrix X using K-means.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): Input feature matrix of shape (n_samples, n_features).\n",
    "        n_clusters (int): Number of clusters.\n",
    "        random_state (int): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Cluster labels for each sample.\n",
    "        KMeans: Fitted KMeans model.\n",
    "    \"\"\"\n",
    "    kmeans = KMeans(\n",
    "        n_clusters=n_clusters,\n",
    "        random_state=random_state,\n",
    "        n_init=10,\n",
    "        max_iter=300\n",
    "    )\n",
    "    predicted_labels = kmeans.fit_predict(X)\n",
    "    return predicted_labels, kmeans"
   ],
   "id": "319020e65f6658fe"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 3: Evaluation Metrics\n",
    "\n",
    "To evaluate the clustering performance, we rely on three standard metrics used in the paper:\n",
    "\n",
    "- **Clustering Accuracy (ACC)**\n",
    "  Measures the best alignment between predicted clusters and ground-truth labels.\n",
    "  Since cluster IDs are arbitrary, the Hungarian algorithm is used to find the optimal mapping.\n",
    "\n",
    "- **Normalized Mutual Information (NMI)**\n",
    "  Measures the mutual dependence between predicted clusters and true labels.\n",
    "  Values range from 0 (no mutual information) to 1 (perfect correlation).\n",
    "\n",
    "- **Adjusted Rand Index (ARI)**\n",
    "  Measures the similarity between two assignments, adjusted for chance.\n",
    "  Values range from -1 to 1, where 1 indicates perfect agreement.\n",
    "\n",
    "These metrics provide complementary views:\n",
    "- **ACC** focuses on label alignment,\n",
    "- **NMI** evaluates information overlap,\n",
    "- **ARI** accounts for random chance.\n"
   ],
   "id": "8eb1d4c3aa45eb53"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def clustering_accuracy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate clustering accuracy using the Hungarian algorithm.\n",
    "\n",
    "    Args:\n",
    "        y_true (np.ndarray): Ground truth labels.\n",
    "        y_pred (np.ndarray): Predicted cluster labels.\n",
    "\n",
    "    Returns:\n",
    "        float: Clustering accuracy.\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true).astype(np.int64) # Ensure integer type\n",
    "    y_pred = np.asarray(y_pred).astype(np.int64)\n",
    "    assert y_pred.size == y_true.size # Ensure same size\n",
    "\n",
    "    D = max(y_pred.max(), y_true.max()) + 1 # Number of clusters\n",
    "    w = np.zeros((D, D), dtype=np.int64)\n",
    "    for i in range(y_pred.size):\n",
    "        w[y_pred[i], y_true[i]] += 1 # Build contingency matrix\n",
    "\n",
    "    # Hungarian algorithm to find optimal assignment\n",
    "    row_ind, col_ind = linear_sum_assignment(w.max() - w) # Maximize accuracy, linear_sum_assignment minimizes cost\n",
    "    mapping = dict(zip(row_ind, col_ind)) # Create mapping\n",
    "\n",
    "    # Map predicted labels to true labels\n",
    "    y_pred_mapped = np.array([mapping[label] for label in y_pred])\n",
    "    accuracy = np.mean(y_pred_mapped == y_true) # Calculate accuracy\n",
    "    return accuracy"
   ],
   "id": "2f3a5f2631bf5b91"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def clustering_nmi(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate Normalized Mutual Information (NMI) between true and predicted labels.\n",
    "\n",
    "    Args:\n",
    "        y_true (np.ndarray): Ground truth labels.\n",
    "        y_pred (np.ndarray): Predicted cluster labels.\n",
    "\n",
    "    Returns:\n",
    "        float: NMI score.\n",
    "    \"\"\"\n",
    "    return normalized_mutual_info_score(y_true, y_pred)"
   ],
   "id": "40814d33a771c057"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def clustering_ari(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate Adjusted Rand Index (ARI) between true and predicted labels.\n",
    "\n",
    "    Args:\n",
    "        y_true (np.ndarray): Ground truth labels.\n",
    "        y_pred (np.ndarray): Predicted cluster labels.\n",
    "\n",
    "    Returns:\n",
    "        float: ARI score.\n",
    "    \"\"\"\n",
    "    return adjusted_rand_score(y_true, y_pred)"
   ],
   "id": "e9412d89c1d2d2d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 4: Feature Visualization\n",
    "\n",
    "To qualitatively assess the quality of the extracted features, we can project the high-dimensional embeddings into a 2D space and visualize them.\n",
    "\n",
    "We use **t-SNE (t-distributed Stochastic Neighbor Embedding)**, which is a nonlinear dimensionality reduction technique that preserves local similarities between points.\n",
    "\n",
    "If the extracted deep features are meaningful, samples from the same class should form compact clusters in the 2D visualization, while different classes should be well separated.\n",
    "\n",
    "This visualization helps to confirm whether the embeddings from ELMo or InferSent provide more discriminative representations compared to traditional methods like tf-idf.\n"
   ],
   "id": "b5cebcc3486cab7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def visualize_features(X, labels, title='Feature Visualization with t-SNE', n_samples=1000, random_state=42):\n",
    "    \"\"\"\n",
    "    Visualize high-dimensional features using t-SNE.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): Input feature matrix of shape (n_samples, n_features).\n",
    "        labels (np.ndarray): Ground truth labels for coloring.\n",
    "        title (str): Title of the plot.\n",
    "        n_samples (int): Number of samples to visualize (for large datasets).\n",
    "        random_state (int): Random seed for reproducibility.\n",
    "\n",
    "    \"\"\"\n",
    "    # Randomly sample a subset for visualization (for large datasets)\n",
    "    if X.shape[0] > n_samples:\n",
    "        np.random.seed(random_state)\n",
    "        indices = np.random.choice(X.shape[0], n_samples, replace=False)\n",
    "        X_sampled = X[indices]\n",
    "        labels_sampled = np.array(labels)[indices]\n",
    "    else:\n",
    "        X_sampled = X\n",
    "        labels_sampled = labels\n",
    "\n",
    "    tsne = TSNE(n_components=2, perplexity=30, learning_rate=200,\n",
    "                n_iter=1000, random_state=random_state, init=\"pca\") # Hyperparameters from paper\n",
    "    X_2d = tsne.fit_transform(X_sampled)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    scatter = plt.scatter(X_2d[:, 0], X_2d[:, 1], c=labels_sampled, cmap='tab10', alpha=0.7)\n",
    "    plt.colorbar(scatter, ticks=np.unique(labels_sampled))\n",
    "    plt.title(title)\n",
    "    plt.xlabel('t-SNE Dimension 1')\n",
    "    plt.ylabel('t-SNE Dimension 2')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ],
   "id": "8f136fb8c9404c4f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
